{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "927a06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM, BitsAndBytesConfig, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8d5fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = TextLoader(\"test.txt\").load()\n",
    "# chunks = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=500,\n",
    "#     chunk_overlap=100\n",
    "# ).split_documents(doc)\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    model_kwargs={\n",
    "        \"device\": \"cpu\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3e2c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "connection = 'postgresql+psycopg://langchain:langchain@localhost:6024/langchain'\n",
    "# db = PGVector.from_documents(\n",
    "#     chunks,\n",
    "#     connection=connection,\n",
    "#     embedding=embedding_model\n",
    "# )\n",
    "\n",
    "db = PGVector(\n",
    "    connection=connection,\n",
    "    embeddings=embedding_model\n",
    ")\n",
    "retriever = db.as_retriever(search_kwargs={\"k\" : 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d96266c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0423b0d4ca04457ba9408cf7d1761db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             device_map=\"auto\",\n",
    "                                             max_memory={\n",
    "                                                 0 : \"3.5GiB\",\n",
    "                                                 \"cpu\" : \"4.5GiB\"\n",
    "                                             })\n",
    "pipe = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=2000\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "795cebac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Answer the question using **only** the following context. Answer it in sharp and short way.\n",
      "\n",
      "Context:\n",
      "PART I: GENERAL PROVISIONS\n",
      "\n",
      "1. Short Title\n",
      "This Directive may be cited as the \"Directive for the Dematerialization of Publicly Offered Securities No. 1047/2025.\"\n",
      "\n",
      "2. Definitions\n",
      "In this Directive unless the context otherwise requires:\n",
      "\n",
      "1/ \"Authority\" means the Ethiopian Capital Market Authority established under the Capital Market Proclamation No. 1248/2021.\n",
      "\n",
      "[file name]: standardized_formatted.pdf\n",
      "[file content begin]\n",
      "\n",
      "ETHIOPIAN CAPITAL MARKET AUTHORITY\n",
      "DIRECTIVE NUMBER 1047/2025\n",
      "\n",
      "DEMATERIALIZATION OF PUBLICLY OFFERED SECURITIES\n",
      "MARCH 2025\n",
      "\n",
      "TABLE OF CONTENTS\n",
      "\n",
      "33. Effective Date\n",
      "This Directive shall come into force on the date of its registration with the Ministry of Justice and uploading it onto the official website of the Authority.\n",
      "\n",
      "DONE IN ADDIS ABABA ON THE 5TH DAY OF MARCH 2025\n",
      "\n",
      "HANA TEHELKU\n",
      "DIRECTOR GENERAL\n",
      "ETHIOPIAN CAPITAL MARKET AUTHORITY\n",
      "\n",
      "[file content end]\n",
      "\n",
      "\n",
      "Question: Who is the leader of ECMA?\n",
      "\n",
      "Assistant: Hana Tehelku is the Director General of the Ethiopian Capital Market Authority (ECMA). Therefore, she is the leader of ECMA. \n",
      "\n",
      "Short answer: Hana Tehelku.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Answer the question using **only** the following context. Answer it in sharp and short way.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | llm\n",
    "\n",
    "\n",
    "def qa(question):\n",
    "    docs = retriever.invoke(question)\n",
    "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "    \n",
    "    results = chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"question\": question\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "answer = qa(\"Who is the leader of ECMA?\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d65247d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Provide a better search query for legal search on\n",
      "                                                  legal information about the Ethiopian Capital Market laws to answer the given question.\n",
      "                                                  End the queries with '**'. Just give me one good query and do it quickly.\n",
      "\n",
      "                                                  Questions: I woke up today in a very weird day. The temperature was hot. \n",
      "I needed a shower so I took one and here I am. Who is the general director of ECMA?\n",
      "                                                  Answer:\n",
      "                                                  The general director of ECMA is Mr. Alemayehu G. Tewolde.\n",
      "\n",
      "AssistantAssistant: \"general director of ECMA Ethiopia capital market authority\"**\n"
     ]
    }
   ],
   "source": [
    "rewrite_prompt = ChatPromptTemplate.from_template(\"\"\"Provide a better search query for legal search on\n",
    "                                                  legal information about the Ethiopian Capital Market laws to answer the given question.\n",
    "                                                  End the queries with '**'. Just give me one good query and do it quickly.\n",
    "\n",
    "                                                  Questions: {x}\n",
    "                                                  Answer:\n",
    "\"\"\")\n",
    "rewriter = rewrite_prompt | llm\n",
    "\n",
    "question = \"\"\"I woke up today in a very weird day. The temperature was hot. \n",
    "I needed a shower so I took one and here I am. Who is the general director of ECMA?\"\"\"\n",
    "\n",
    "result = rewriter.invoke({\"x\" : question})\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
